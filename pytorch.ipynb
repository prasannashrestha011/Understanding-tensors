{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOlTQBhXwl/PQUwVRa9fQOM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasannashrestha011/Understanding-tensors/blob/main/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ6l7U_PAXb4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from d2l import torch as d2l\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ArGjIIboAkDa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "27b2629a-2637-420f-e1f8-763feb2c0d6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'd2l'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1292524989.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0md2l\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'd2l'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##adding up column to the row,resulting tensor in one D\n",
        "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "X.sum(axis=1)"
      ],
      "metadata": {
        "id": "IoAvjfkMAoK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obtaining mean of tensor\n",
        "X.sum()/X.numel()"
      ],
      "metadata": {
        "id": "OabSl-gOBLt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#non reduction form of dimension by adding up rows and cols\n",
        "X.sum(axis=0,keepdim=True)"
      ],
      "metadata": {
        "id": "sZZNiBoKB2bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.tensor([[1,2],[3,4]])\n",
        "A_sum=A.sum(axis=1,keepdim=True)\n",
        "normalized_A=A/A_sum\n",
        "normalized_A"
      ],
      "metadata": {
        "id": "ZILMa1hXCCJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(\"Row cumulative sum= \",A.cumsum(axis=0))\n",
        "print(\"Column cumulative sum: \\n \",A.cumsum(axis=1))"
      ],
      "metadata": {
        "id": "dlRVbugsDiGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.tensor([1,2,3,4])\n",
        "Y=torch.tensor([5,6,7,8])\n",
        "print(torch.dot(X,Y))\n",
        "print(torch.sum(X*Y))"
      ],
      "metadata": {
        "id": "DPiyhAy_JOec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MYb4ztWYPFRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float32)\n",
        "x=torch.tensor([1,2,3],dtype=torch.float32)\n",
        "y=torch.zeros(2,dtype=torch.float32)\n",
        "y[0]=torch.dot(A[0],x)\n",
        "y[1]=torch.dot(A[1],x)\n",
        "y\n",
        "\n"
      ],
      "metadata": {
        "id": "AH7EhicHNb9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.ones(2,1)\n",
        "B=torch.ones(1,2)\n",
        "A,B,torch.matmul(A,B)"
      ],
      "metadata": {
        "id": "ZaMaYGB4Slu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([1,2,100],dtype=torch.float32)\n",
        "l1=torch.abs(x).sum()\n",
        "l2=torch.norm(x)\n",
        "l1,l2"
      ],
      "metadata": {
        "id": "g3M8bN30R3my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transpose of A.T.T=A\n",
        "A=torch.tensor([[1,2,3],[4,5,6]])\n",
        "A_T=A.T\n",
        "A_T_T=A_T.T\n",
        "A,A_T,A_T_T"
      ],
      "metadata": {
        "id": "3t4erQpKaEny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A^T+B^T=(A+B)^T\n",
        "A=torch.rand(4,4,dtype=torch.float32)\n",
        "B=torch.rand(4,4,dtype=torch.float32)\n",
        "A_T=A.T\n",
        "B_T=B.T\n",
        "A_T+B_T,(A+B).T"
      ],
      "metadata": {
        "id": "oKrKDWPUabEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.rand(3,3,dtype=torch.float32)\n",
        "A_T=A.T\n",
        "A, A+A_T"
      ],
      "metadata": {
        "id": "m7xTvNaOlSbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.randn(1,2,3,4)\n",
        "len(X)"
      ],
      "metadata": {
        "id": "W0Eq6QgAmcxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.tensor([[1,2,3],[4,5,6]])\n",
        "X/X.sum(axis=1,keepdim=True)\n"
      ],
      "metadata": {
        "id": "8sBMoUi8oLD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# First backward pass\n",
        "y = x * x         # y = xÂ²\n",
        "y.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "or1TfEwn17yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dervative\n",
        "x=torch.tensor(3.0,requires_grad=True)\n",
        "y=x**2\n",
        "y.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "-wQsHqPS2k0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "XBDu4CQFwVtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normal(x, mu, sigma):\n",
        "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
        "    return p * np.exp(-0.5 * (x - mu)**2 / sigma**2)"
      ],
      "metadata": {
        "id": "bjerQKjJwcdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use NumPy again for visualization\n",
        "x = np.arange(-9, 8, 0.01)\n",
        "\n",
        "# Mean and standard deviation pairs\n",
        "params = [(1, 1), (0, 2), (3, 1)]\n",
        "d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',\n",
        "         ylabel='p(x)', figsize=(4.5, 2.5),\n",
        "         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])"
      ],
      "metadata": {
        "id": "5iIjnJ9ow95P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Device config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Expanded training data for better learning\n",
        "pairs = [\n",
        "    (\"hi\", \"hello there\"),\n",
        "    (\"hello\", \"hi how are you\"),\n",
        "    (\"how are you\", \"i am fine thank you\"),\n",
        "    (\"how are you doing\", \"i am doing well\"),\n",
        "    (\"what is your name\", \"i am a chatbot\"),\n",
        "    (\"who are you\", \"i am an ai assistant\"),\n",
        "    (\"bye\", \"goodbye see you later\"),\n",
        "    (\"goodbye\", \"bye take care\"),\n",
        "    (\"thanks\", \"you are welcome\"),\n",
        "    (\"thank you\", \"no problem\"),\n",
        "    (\"what can you do\", \"i can chat with you\"),\n",
        "    (\"help\", \"i am here to help you\"),\n",
        "    (\"yes\", \"okay great\"),\n",
        "    (\"no\", \"i understand\"),\n",
        "    (\"\")\n",
        "    (\"maybe\", \"that sounds reasonable\"),\n",
        "    (\"good morning\", \"good morning to you\"),\n",
        "    (\"good night\", \"good night sleep well\"),\n",
        "    (\"how old are you\", \"i am a computer program\"),\n",
        "    (\"where are you from\", \"i exist in the digital world\"),\n",
        "    (\"what do you like\", \"i like helping people\"),\n",
        "]\n",
        "\n",
        "# Build vocab from pairs\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2index = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
        "        self.index2word = {0:\"<pad>\", 1:\"<sos>\", 2:\"<eos>\", 3:\"<unk>\"}\n",
        "        self.n_words = 4\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for w in sentence.lower().split():\n",
        "            self.add_word(w)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            idx = self.n_words\n",
        "            self.word2index[word] = idx\n",
        "            self.index2word[idx] = word\n",
        "            self.n_words += 1\n",
        "\n",
        "    def sentence_to_indexes(self, sentence):\n",
        "        return [self.word2index.get(w, 3) for w in sentence.lower().split()] + [2]  # UNK for unknown words, EOS at end\n",
        "\n",
        "    def indexes_to_sentence(self, indexes):\n",
        "        return ' '.join([self.index2word.get(i, \"<unk>\") for i in indexes if i not in [0,1,2]])\n",
        "\n",
        "input_vocab = Vocab()\n",
        "output_vocab = Vocab()\n",
        "\n",
        "for p in pairs:\n",
        "    input_vocab.add_sentence(p[0])\n",
        "    output_vocab.add_sentence(p[1])\n",
        "\n",
        "print(f\"Input vocabulary size: {input_vocab.n_words}\")\n",
        "print(f\"Output vocabulary size: {output_vocab.n_words}\")\n",
        "\n",
        "# Encoder with larger hidden size\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# Attention Decoder\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length=15):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attn = nn.Linear(hidden_size*2, max_length)\n",
        "        self.attn_combine = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attn_input = torch.cat((embedded[0], hidden[0]), 1)\n",
        "        attn_weights = F.softmax(self.attn(attn_input), dim=1)\n",
        "\n",
        "        # Apply attention\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        # Combine and process\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "# Training parameters\n",
        "teacher_forcing_ratio = 0.5\n",
        "max_length = 15\n",
        "criterion = nn.NLLLoss()\n",
        "learning_rate = 0.001  # Lower learning rate for better stability\n",
        "\n",
        "# Larger hidden size for better capacity\n",
        "hidden_size = 64\n",
        "encoder = EncoderRNN(input_vocab.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_vocab.n_words, max_length).to(device)\n",
        "\n",
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "def tensors_from_sentence(vocab, sentence):\n",
        "    indexes = vocab.sentence_to_indexes(sentence)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device)\n",
        "\n",
        "def train(input_tensor, target_tensor):\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "    loss = 0\n",
        "\n",
        "    # Encode\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        if ei < max_length:\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    # Decode\n",
        "    decoder_input = torch.tensor([1], device=device)  # SOS\n",
        "    decoder_hidden = encoder_hidden\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
        "            decoder_input = target_tensor[di]\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
        "            if decoder_input.item() == 2:  # EOS\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / target_length\n",
        "\n",
        "def evaluate(sentence):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensors_from_sentence(input_vocab, sentence)\n",
        "        input_length = input_tensor.size(0)\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        # Encode\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            if ei < max_length:\n",
        "                encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "        # Decode\n",
        "        decoder_input = torch.tensor([1], device=device)  # SOS\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            if topi.item() == 2:  # EOS\n",
        "                break\n",
        "            elif topi.item() != 0:  # Skip PAD tokens\n",
        "                decoded_words.append(output_vocab.index2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return ' '.join(decoded_words) if decoded_words else \"i don't understand\"\n",
        "\n",
        "# Training\n",
        "n_epochs = 5000\n",
        "print_every = 500\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    pair = random.choice(pairs)\n",
        "    input_tensor = tensors_from_sentence(input_vocab, pair[0])\n",
        "    target_tensor = tensors_from_sentence(output_vocab, pair[1])\n",
        "    loss = train(input_tensor, target_tensor)\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print(f\"Epoch {epoch} Loss {loss:.4f}\")\n",
        "\n",
        "print(\"\\nChatbot ready! Type 'quit' to stop.\")\n",
        "print(\"Try: hi, how are you, what is your name, bye\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "    try:\n",
        "        output = evaluate(user_input)\n",
        "        print(\"Bot:\", output)\n",
        "    except Exception as e:\n",
        "        print(f\"Bot: Sorry, I had an error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH3yXu5y7GOw",
        "outputId": "7deb73ba-4f0c-40ed-d55b-fb93144bee8f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input vocabulary size: 32\n",
            "Output vocabulary size: 56\n",
            "\n",
            "Starting training...\n",
            "Epoch 500 Loss 1.1001\n",
            "Epoch 1000 Loss 0.0480\n",
            "Epoch 1500 Loss 0.0409\n",
            "Epoch 2000 Loss 0.0132\n",
            "Epoch 2500 Loss 0.0081\n",
            "Epoch 3000 Loss 0.0082\n",
            "Epoch 3500 Loss 0.0042\n",
            "Epoch 4000 Loss 0.0027\n",
            "Epoch 4500 Loss 0.0039\n",
            "Epoch 5000 Loss 0.0030\n",
            "\n",
            "Chatbot ready! Type 'quit' to stop.\n",
            "Try: hi, how are you, what is your name, bye\n",
            "\n",
            "You: what \n",
            "Bot: i am here to help you\n",
            "\n",
            "You: how are you\n",
            "Bot: i am fine thank you\n",
            "\n",
            "You: i am gay\n",
            "Bot: i am doing to help you\n",
            "\n",
            "You: my name is prasanna\n",
            "Bot: okay great\n",
            "\n",
            "You: i feel sad\n",
            "Bot: goodbye see you later\n",
            "\n",
            "You: whats up\n",
            "Bot: goodbye see you later\n",
            "\n",
            "You: what the heck\n",
            "Bot: i am a chatbot\n",
            "\n",
            "You: who are yo\n",
            "Bot: i am an ai assistant\n",
            "\n",
            "You: what do you do\n",
            "Bot: i like helping people\n",
            "\n",
            "You: you are a ai\n",
            "Bot: i am fine thank you\n",
            "\n",
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}